---
title: "Theming with bslib and thematic"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      secondary: "#00DAC6"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
---

```{r, prompt=FALSE}
library(tidyverse)
library(tidymodels)
library(janitor)

```

```{r, prompt=F}
df <- read_csv("/home/norhther/Documentos/GitHub/MVA-Project/df_preprocessed.csv")
```


We can try to perform a Decision Tree to predict the Place of the lifter -> 1, 2, 3 or Other 

```{r}

df %>% 
  mutate(Place = fct_lump(Place, 3)) %>% 
  count(Place)

df <- df %>%
  mutate(Place = fct_lump(Place, 3))

```

Create a train/test split of 75% for training, stratifying by Place
```{r}
set.seed(42)

df_selected <- df %>% select(Sex, Age, BodyweightKg, WeightTriedSquat1Kg, WeightTriedSquat2Kg, WeightTriedBench1Kg, WeightTriedSquat1Kg,
              WeightTriedSquat2Kg, WeightTriedDeadlift1Kg, WeightTriedDeadlift2Kg, `LiftedSquat1Kg?`,
              `LiftedSquat2Kg?`, `LiftedBench1Kg?`, `LiftedBench2Kg?`,
              `LiftedDeadlift1Kg?`, `LiftedDeadlift2Kg?`, Place) %>%
  mutate_if(is.character, as_factor) %>%
  clean_names()

df_split <- initial_split(df_selected, strata = place)
df_train <- training(df_split)
df_test <- testing(df_split)

```


And a 10 folds for cv, also stratifying by Place
```{r}
set.seed(42)
df_folds <- vfold_cv(df_train, strata = place)
df_folds

```


Model specification: tune cost_complexity, tree_depth and min_n
Create a grid of parameters
```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = 4,
  min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_spec

tree_grid <- grid_latin_hypercube(cost_complexity(), min_n(), size = 10)

tree_grid
```

```{r}
doParallel::registerDoParallel()

set.seed(42)
tree_rs <- tune_grid(
  tree_spec,
  place ~ .,
  resamples = df_folds,
  grid = tree_grid,
  metrics = metric_set(accuracy, precision, recall)
)

tree_rs
```



```{r}
collect_metrics(tree_rs)

autoplot(tree_rs) + theme_light()

show_best(tree_rs, "accuracy")
```

There are several best combinations depending of which metric we want to use, and also
within the same metric. We are going to use select_best to select one and finalize the 
model
```{r}

final_tree <- finalize_model(tree_spec, select_best(tree_rs, "accuracy"))
final_tree

```

```{r}
final_fit <- fit(final_tree, place ~ ., df_train)
final_rs <- last_fit(final_tree, place  ~ ., df_split)
```



Show the metrics for the test split
```{r}
library(patchwork)
library("RColorBrewer")

collect_metrics(final_rs)

cm <- final_rs %>%
  collect_predictions() %>%
  conf_mat(place, .pred_class)

autoplot(cm, type = "heatmap")
```



Check the importance of each predictor in the model
```{r}
library(vip)

final_fit %>%
  vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8))
```


We can also try to visualize a tree with some of the most important features
to see what the model is doing
```{r}
library(parttree)

ex_fit <- fit(
  final_tree,
  place ~ age + bodyweight_kg,
  df_train
)

df_train %>%
  ggplot(aes(age, bodyweight_kg)) +
  geom_parttree(data = ex_fit, aes(fill = turbine_capacity), alpha = 0.3) +
  geom_jitter(alpha = 0.7, width = 1, height = 0.5, aes(color = turbine_capacity)) +
  scale_colour_viridis_c(aesthetics = c("color", "fill"))
```
```{r}

```



Save the model
```{r}
#save(final_rs, file = "C:\\Users\\omarl\\OneDrive\\Escritorio\\MVA-Project\\model_fitted.Rdata")
```

Plot the tree for interpretation
```{r}
library(rpart)
library(rpart.plot)


rpart.plot(final_fit$fit)

```


