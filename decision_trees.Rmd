---
title: "Theming with bslib and thematic"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "#202123"
      fg: "#B8BCC2"
      primary: "#EA80FC"
      secondary: "#00DAC6"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
---

```{r, prompt=FALSE}
library(tidyverse)
library(tidymodels)
library(janitor)
library(recipes)
library(modeldata)
library(themis)
library(caret)
library(cvms)
```

```{r, prompt=F}
df <- read_csv("C:\\Users\\omarl\\OneDrive\\Escritorio\\MVA-Project\\df_preprocessed.csv")
```


We can try to perform a Decision Tree to predict the Place of the lifter -> 1, 2, 3 or Other 

```{r}

df %>% 
  mutate(Place = fct_lump(Place, 3)) %>% 
  count(Place)

df <- df %>%
  mutate(Place = fct_lump(Place, 3))

```

Create a train/test split of 75% for training, stratifying by Place
We use a smote algorithm to generate synthetic data to avoid
overfitting on majority classes.
```{r}
set.seed(42)

df_selected <- df %>% select(Sex, Age, BodyweightKg, WeightTriedSquat1Kg, WeightTriedSquat2Kg, WeightTriedBench1Kg, WeightTriedSquat1Kg,
              WeightTriedSquat2Kg, WeightTriedDeadlift1Kg, WeightTriedDeadlift2Kg, `LiftedSquat1Kg?`,
              `LiftedSquat2Kg?`, `LiftedBench1Kg?`, `LiftedBench2Kg?`,
              `LiftedDeadlift1Kg?`, `LiftedDeadlift2Kg?`, Place) %>%
  mutate_if(is.character, as_factor) %>%
  clean_names()

df_split <- initial_split(df_selected, strata = place)
df_train <- training(df_split)
df_test <- testing(df_split)

up_rec <- recipe(place ~ ., data = df_train) %>%
  step_dummy(all_factor_predictors()) %>%
  step_smote(place) %>%
  prep()

```


```{r}
df_train <- juice(up_rec)

#adjusting smote
df_train <- df_train %>%
  mutate(sex_M = ifelse(sex_M < 0.5, 0, 1),
         across(8:13, ~ ifelse(.x < 0.5, 0, 1), .names = "{.col}"))
  
df_train %>%
  count(place)
```


And a 10 folds for cv, also stratifying by Place
```{r}
set.seed(42)
df_folds <- vfold_cv(df_train, strata = place)
df_folds
```


Model specification: tune cost_complexity, tree_depth and min_n
Create a grid of parameters
```{r}
tree_spec <- decision_tree(
  cost_complexity = tune(),
  tree_depth = tune(),
  min_n = tune()) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_spec

tree_grid <- grid_latin_hypercube(cost_complexity(), min_n(),tree_depth(), size = 50)

tree_grid
```



```{r}
doParallel::registerDoParallel()

set.seed(42)
tree_rs <- tune_grid(
  tree_spec,
  place ~ .,
  resamples = df_folds,
  grid = tree_grid,
  metrics = metric_set(accuracy, yardstick::precision, yardstick::recall)
)

tree_rs
```



```{r}
collect_metrics(tree_rs)

# save(tree_rs, file = "C:\\Users\\omarl\\OneDrive\\Escritorio\\MVA-Project\\tree_rs.Rdata")

autoplot(tree_rs) + theme_light()

show_best(tree_rs, "accuracy")

```

There are several best combinations depending of which metric we want to use, and also
within the same metric. We are going to use select_best to select one and finalize the 
model
```{r}

final_tree <- finalize_model(tree_spec, select_best(tree_rs, "accuracy"))
final_tree

```

```{r}
final_fit <- fit(final_tree, place ~ ., df_train)

#final_rs <- last_fit(final_tree, place  ~ ., df_split)
```

```{r}
train_preds <- predict(final_fit, 
        df_train)
cm <- confusionMatrix(train_preds$.pred_class, df_train$place)
cm

plot_confusion_matrix(as_tibble(cm$table),
                      target_col = "Reference",
                      prediction_col = "Prediction",
                      counts_col = "n",
                      add_row_percentages = F,
                      add_col_percentages = F,
                      palette = "Reds")
```


Show the metrics for the test split
```{r}

df_test2 <- bake(up_rec, df_test)

test_preds <- predict(final_fit, 
        df_test2)

cm <- confusionMatrix(test_preds$.pred_class, df_test2$place)
cm

plot_confusion_matrix(as_tibble(cm$table),
                      target_col = "Reference",
                      prediction_col = "Prediction",
                      counts_col = "n",
                      add_row_percentages = F,
                      add_col_percentages = F,
                      palette = "Reds")

```



Check the importance of each predictor in the model
```{r}
library(vip)

final_fit %>%
  vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8))
```


Save the model
```{r}
save(final_fit, file = "C:\\Users\\omarl\\OneDrive\\Escritorio\\MVA-Project\\model_fitted.Rdata")
```

Plot the tree for interpretation
```{r}
library(rpart)
library(rpart.plot)


rpart.plot(final_fit$fit)

```


